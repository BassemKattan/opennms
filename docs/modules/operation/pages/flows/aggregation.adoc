
[[ga-flow-support-aggregation]]
= Aggregation and Streaming Analytics

Large amounts of flows data may result in slow generation of dashboard visualizations, particularly with longer time ranges (last day/week).
A lot of calculations must occur to render these dashboards, which is challenging to do in real-time with large volumes of flow data.

OpenNMS Nephron is a streaming application that reads flows from Kafka, calculates flow aggregates, and persists these aggregates in Elasticsearch, Cortex, or Kafka.
It pre-computes data as it flows through the pipeline, and stores it in Elasticsearch.

Queries from dashboards are significantly faster (under 10 seconds vs. 30 minutes on billions of documents), as the flow query engine renders Top-N metrics from these pre-aggregated documents stored in Elasticsearch.
Nephron also helps alleviate compute load on the Elasticsearch cluster, particularly for environments with large volumes of flows (>10,000 flows/sec).

== Before you begin

Make sure you have the following:

* Horizon 26/Meridian 2020 or later
* xref:operation:flows/basic.adoc#flows-basic[Basic flows environment set up]
* xref:operation:flows/distributed.adoc#flows-remote[Minion added to your flows environment]]
* xref:operation:flows/sentinel/sentinel.adoc#flows-scaling[Sentinel added to your flows environment]
* link:https://flink.apache.org/[Apache Flink] cluster set up and sized to your flows volume
* Kafka server with xref:operation:flows/aggregation.adoc#kafka-forwarder-config[forwarder enabled]
* A fast-caching DNS resolver if using reverse DNS

[[kafka-forwarder-config]]
== Enable Kafka forwarder

To use Nephron, you must enable the Kafka forwarder to receive enriched flows (with OpenNMS node data).

Enriched flows are stored in the `flowDocuments` topic and the payloads are encoded using link:https://developers.google.com/protocol-buffers/[Google Protocol Buffers (GPB)].
See `flowdocument.proto` in the corresponding source distribution for the model definitions.

Enable Kafka forwarding:
[source, console]
----
$ ssh -p 8101 admin@localhost
...
config:edit org.opennms.features.flows.persistence.elastic
config:property-set enableForwarding true
config:update
----

Configure Kafka server for flows:

[source, console]
----
$ ssh -p 8101 admin@localhost
...
config:edit org.opennms.features.flows.persistence.kafka
config:property-set bootstrap.servers 127.0.0.1:9092
config:update
----

=== Correct clock skew

Flow analyses use timestamps exposed by the underlying flow management protocol.
These timestamps are set depending on the clock of the exporting router.
If the router's clock differs from the actual time, this will be reflected in received flows and therefore skew up further analysis and aggregation.

{page-component-title} Core can correct the timestamps of a received flow.
To do so, it compares the current time of the exporting device with the actual time when the packet has been received.
If these times differ by a certain amount, OpenNMS considers the receive time more correct and all timestamps of the flow will be adapted.

To enable clock correction, configure a threshold for the maximum allowed delta in milliseconds.
Set the threshold to `0` to disable the correction mechanism.

[source, console]
----
$ ssh -p 8101 admin@localhost
...
config:edit org.opennms.features.flows.persistence.elastic
config:property-set clockSkewCorrectionThreshold 5000
config:update
----

== Configure flow input from Kafka

Use the following command line arguments to configure flow input from Kafka:

[source, console]
----
--bootstrapServers=...
--flowSourceTopic=...
----

The `flowSourceTopic` given here must match the topic configured for the xref:operation:flows/aggregation.adoc#kafka-forwarder-config[Kafka forwarder].

== Elasticsearch persistence

Use the following command line argument to configure Elasticsearch persistence:

[source, console]
----
--elasticUrl=http://<server>:9200
----

For additional options, see link:https://github.com/OpenNMS/nephron/blob/master/main/src/main/java/org/opennms/nephron/NephronOptions.java[NephronOptions].

If no Elasticsearch output is required, then set `--elasticUrl` as empty.

== Cortex persistence

Use the following command line argument to configure Cortex persistence:

[source, console]
----
--cortexWriteUrl=http://<server>:9009/api/v1/push
----

For additional options, see link:https://github.com/OpenNMS/nephron/blob/master/main/src/main/java/org/opennms/nephron/CortexOptions.java[CortexOptions].

== Kafka persistence

Use the following command line argument to configure Kafka persistence:

[source, console]
----
--flowDestTopc=...
----

== Monitoring

Nephron provides a number of metrics you can use to monitor its operation.
If you use Prometheus to monitor Nephron, you must enable the link:https://ci.apache.org/projects/flink/flink-docs-master/docs/deployment/metric_reporters/#prometheus[Prometheus metric reporter] by adding the following lines to `flink-conf.yaml`:

[source, console]
----
metrics.reporter.prom.class: org.apache.flink.metrics.prometheus.PrometheusReporter
metrics.reporter.prom.port: 9250-9260
----

You must add a corresponding scrape configuration to `prometheus.yml`:

[source, console]
----
scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  - job_name: 'flink'
    static_configs:
    - targets: ['localhost:9250', 'localhost:9251', 'localhost:9252', 'localhost:9253', 'localhost:9254', 'localhost:9255', 'localhost:9256', 'localhost:9257', 'localhost:9258', 'localhost:9259', 'localhost:9260']
      labels:
      flinklabel: 'flink'
----

=== Metrics

[options="header" cols="1,2,1,4"]
|===
| Namespace
| Name
| Type
| Description

| flows
| from_kafka
| Counter
| The number of flows read from Kafka.

| flows
| from_kafka_drift
| Gauge
| The number of milliseconds between processing time and the `last_switched` of flows.

| flows
| to_es
| Counter
| The number of flows written to Elasticsearch.

| cortex
| write
| Counter
| The number of batches written to Cortex.

| cortex
| sample
| Counter
| The number of Cortex samples.

| cortex
| write_failure
| Counter
| The number of Cortex write failures (writes that completed with an exception).

| cortex
| response_failure
| Counter
| The number of Cortex response failures (responses to Cortex writes that indicate a failure condition).

| cortex
| response_failure_<kind>
| Counter
| The number of Cortex response failures of certain kinds. 
Cortex persistence analyses response failures and tries to derive corresponding failure kinds (e.g., "out of order sample" or "per-user series limit reached").
|===

[options="header" cols="1,2,5"]
|===
| Namespace
| Name
| Prometheus metric name

| flows
| from_kafka
| `flink_taskmanager_job_task_operator_flows_from_kafka`

| flows
| to_es
| `flink_taskmanager_job_task_operator_flows_to_es`

| cortex
| write
| `flink_taskmanager_job_task_operator_cortex_write`

| cortex
| sample
| `flink_taskmanager_job_task_operator_cortex_sample`

| cortex
| write_failure
| `flink_taskmanager_job_task_operator_cortex_write_failure`

| cortex
| response_failure
| `flink_taskmanager_job_task_operator_cortex_response_failure`

| cortex
| response_failure_<kind>
| `{__name__=~"flink_taskmanager_job_.\*response_failure_.*"}`

|===

To use this functionality you must enable the Kafka forwarder as described in <<flows/setup.adoc#kafka-forwarder-config, Configure Kafka forwarder>> and set up link:https://github.com/OpenNMS/nephron[Nephron] to process the flows.


Set the following properties in `$OPENNMS_HOME/etc/org.opennms.features.flows.persistence.elastic.cfg` to control the query engine to use aggregated flows:

.Optional parameters for flow aggregation
[options="header" cols="2,3,1"]
|===
| Property
| Description
| Default

| alwaysUseRawForQueries
| Use raw flow documents to respond to all queries instead of aggregated flows.
| true

| alwaysUseAggForQueries
| Use aggregated flow documents to respond to all queries instead of raw flows.
| false

| timeRangeDurationAggregateThresholdMs
| Queries with time range filters that have a duration greater than this value will use aggregated flows when possible.
| 120000 (2 minutes)

| timeRangeEndpointAggregateThresholdMs
| Queries with time range filters that have an endpoint that is older than this value will use aggregated flows when possible.
| 604800000 (7 days)
|===
